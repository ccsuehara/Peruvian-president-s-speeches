{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "This notebook reads the `TXT` files of the speeches and builds a single dataframe with every tokenized and normalized content we'll use.\n",
    "\n",
    "Please note that none of the code chunks of this notebook were actually ran from here. As the processing part took a great amount of time to be completed, we transformed this notebook in a `Python` script and submitted it through the slurm work manager of the Computer Science department. The script is the file `2_data_preparation.py` in this same directory, and the file `run_data_preparation.sbatch` loads it to the slurm environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#from lucem_illud_2020 import word_tokenize\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../../scripts')\n",
    "import data_cleaning as clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the directory of the speeches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_dir = '../../../data/presidentialSpeechPeru/txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the corpus from this path using a helper function we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mensaje-1963-fbt.txt\n",
      "mensaje-1964-fbt.txt\n",
      "mensaje-1965-fbt.txt\n",
      "mensaje-1966-fbt.txt\n",
      "mensaje-1967-fbt.txt\n",
      "mensaje-1968-fbt.txt\n",
      "mensaje-1969-jva.txt\n",
      "mensaje-1970-jva.txt\n",
      "mensaje-1971-jva.txt\n",
      "mensaje-1972-jva.txt\n",
      "mensaje-1973-jva.txt\n",
      "mensaje-1974-jva.txt\n",
      "mensaje-1975-jva.txt\n",
      "mensaje-1976-fmb.txt\n",
      "mensaje-1977-fmb.txt\n",
      "mensaje-1978-fmb.txt\n",
      "mensaje-1979-fmb.txt\n",
      "mensaje-1980-fbt.txt\n",
      "mensaje-1981-fbt.txt\n",
      "mensaje-1982-fbt.txt\n",
      "mensaje-1983-fbt.txt\n",
      "mensaje-1984-fbt.txt\n",
      "mensaje-1985-ag.txt\n",
      "mensaje-1986-ag.txt\n",
      "mensaje-1987-ag.txt\n",
      "mensaje-1988-ag.txt\n",
      "mensaje-1989-ag.txt\n",
      "mensaje-1990-af.txt\n",
      "mensaje-1991-af.txt\n",
      "mensaje-1992-af.txt\n",
      "mensaje-1993-af.txt\n",
      "mensaje-1994-af.txt\n",
      "mensaje-1995-af.txt\n",
      "mensaje-1996-af.txt\n",
      "mensaje-1997-af.txt\n",
      "mensaje-1998-af.txt\n",
      "mensaje-1999-af.txt\n",
      "mensaje-2000-af.txt\n",
      "mensaje-2000-vp-noviembre.txt\n",
      "mensaje-2001-at.txt\n",
      "mensaje-2002-at.txt\n",
      "mensaje-2003-at.txt\n",
      "mensaje-2004-at.txt\n",
      "mensaje-2005-at.txt\n",
      "mensaje-2006-ag.txt\n",
      "mensaje-2007-ag.txt\n",
      "mensaje-2008-ag.txt\n",
      "mensaje-2009-ag.txt\n",
      "mensaje-2010-ag.txt\n",
      "mensaje-2011-oh.txt\n",
      "mensaje-2012-oh.txt\n",
      "mensaje-2013-oh.txt\n",
      "mensaje-2014-oh.txt\n",
      "mensaje-2015-oh.txt\n",
      "mensaje-2016-ppk.txt\n",
      "mensaje-2017-ppk.txt\n",
      "mensaje-2018-2.txt\n",
      "mensaje-2018-4.txt\n",
      "mensaje-2019-01-vizcarra.txt\n",
      "mensaje-2020-fsh.txt\n"
     ]
    }
   ],
   "source": [
    "speeches_raw = clean.loadcorpus(speeches_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we load the result in a data frame and start adding some metadata columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mensaje-1963-fbt.txt\n",
      "mensaje-1964-fbt.txt\n",
      "mensaje-1965-fbt.txt\n",
      "mensaje-1966-fbt.txt\n",
      "mensaje-1967-fbt.txt\n",
      "mensaje-1968-fbt.txt\n",
      "mensaje-1969-jva.txt\n",
      "mensaje-1970-jva.txt\n",
      "mensaje-1971-jva.txt\n",
      "mensaje-1972-jva.txt\n",
      "mensaje-1973-jva.txt\n",
      "mensaje-1974-jva.txt\n",
      "mensaje-1975-jva.txt\n",
      "mensaje-1976-fmb.txt\n",
      "mensaje-1977-fmb.txt\n",
      "mensaje-1978-fmb.txt\n",
      "mensaje-1979-fmb.txt\n",
      "mensaje-1980-fbt.txt\n",
      "mensaje-1981-fbt.txt\n",
      "mensaje-1982-fbt.txt\n",
      "mensaje-1983-fbt.txt\n",
      "mensaje-1984-fbt.txt\n",
      "mensaje-1985-ag.txt\n",
      "mensaje-1986-ag.txt\n",
      "mensaje-1987-ag.txt\n",
      "mensaje-1988-ag.txt\n",
      "mensaje-1989-ag.txt\n",
      "mensaje-1990-af.txt\n",
      "mensaje-1991-af.txt\n",
      "mensaje-1992-af.txt\n",
      "mensaje-1993-af.txt\n",
      "mensaje-1994-af.txt\n",
      "mensaje-1995-af.txt\n",
      "mensaje-1996-af.txt\n",
      "mensaje-1997-af.txt\n",
      "mensaje-1998-af.txt\n",
      "mensaje-1999-af.txt\n",
      "mensaje-2000-af.txt\n",
      "mensaje-2000-vp-noviembre.txt\n",
      "mensaje-2001-at.txt\n",
      "mensaje-2002-at.txt\n",
      "mensaje-2003-at.txt\n",
      "mensaje-2004-at.txt\n",
      "mensaje-2005-at.txt\n",
      "mensaje-2006-ag.txt\n",
      "mensaje-2007-ag.txt\n",
      "mensaje-2008-ag.txt\n",
      "mensaje-2009-ag.txt\n",
      "mensaje-2010-ag.txt\n",
      "mensaje-2011-oh.txt\n",
      "mensaje-2012-oh.txt\n",
      "mensaje-2013-oh.txt\n",
      "mensaje-2014-oh.txt\n",
      "mensaje-2015-oh.txt\n",
      "mensaje-2016-ppk.txt\n",
      "mensaje-2017-ppk.txt\n",
      "mensaje-2018-2.txt\n",
      "mensaje-2018-4.txt\n",
      "mensaje-2019-01-vizcarra.txt\n",
      "mensaje-2020-fsh.txt\n"
     ]
    }
   ],
   "source": [
    "speech = pd.DataFrame()\n",
    "filenames = []\n",
    "raw = []\n",
    "for filename, raw_speech in speeches_raw.items():\n",
    "    print(filename)\n",
    "    filenames.append(filename)\n",
    "    raw.append(raw_speech)\n",
    "speech['filename'] = filenames\n",
    "speech['raw text'] = raw\n",
    "\n",
    "pattern = re.compile('[0-9]{4}')\n",
    "speech['year'] = speech['filename'].apply(lambda x: pattern.search(x).group(0))\n",
    "speech = speech.sort_values(by='year').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we clean these raw texts using another ad-hoc function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech['cleaned text'] = speech['raw text'].apply(lambda x: clean.clean_raw_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the administration and president of each speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech.loc[(speech['year'].astype('int32') >= 1956) & \\\n",
    "           (speech['year'].astype('int32') <= 1961), 'administration'] = 'Prado'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1962) & \\\n",
    "           (speech['year'].astype('int32') <= 1962), 'administration'] = 'Lindley'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1963) & \\\n",
    "           (speech['year'].astype('int32') <= 1968), 'administration'] = 'Belaunde(1)'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1969) & \\\n",
    "           (speech['year'].astype('int32') <= 1975), 'administration'] = 'Velasco'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1976) & \\\n",
    "           (speech['year'].astype('int32') <= 1979), 'administration'] = 'Morales Bermudez'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1980) & \\\n",
    "           (speech['year'].astype('int32') <= 1984), 'administration'] = 'Belaunde(2)'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1985) & \\\n",
    "           (speech['year'].astype('int32') <= 1989), 'administration'] = 'Garcia(1)'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1990) & \\\n",
    "           (speech['year'].astype('int32') <= 1994), 'administration'] = 'Fujimori(1)'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1995) & \\\n",
    "           (speech['year'].astype('int32') <= 2000), 'administration'] = 'Fujimori(2)'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2001) & \\\n",
    "           (speech['year'].astype('int32') <= 2005), 'administration'] = 'Toledo'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2006) & \\\n",
    "           (speech['year'].astype('int32') <= 2010), 'administration'] = 'Garcia(2)'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2011) & \\\n",
    "           (speech['year'].astype('int32') <= 2015), 'administration'] = 'Humala'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2016), 'administration'] = 'Kuzcynski/Vizcarra'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1956) & \\\n",
    "           (speech['year'].astype('int32') <= 1961), 'president'] = 'Prado'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1962) & \\\n",
    "           (speech['year'].astype('int32') <= 1962), 'president'] = 'Lindley'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1963) & \\\n",
    "           (speech['year'].astype('int32') <= 1968), 'president'] = 'Belaunde'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1969) & \\\n",
    "           (speech['year'].astype('int32') <= 1975), 'president'] = 'Velasco'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1976) & \\\n",
    "           (speech['year'].astype('int32') <= 1979), 'president'] = 'Morales Bermudez'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1980) & \\\n",
    "           (speech['year'].astype('int32') <= 1984), 'president'] = 'Belaunde'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1985) & \\\n",
    "           (speech['year'].astype('int32') <= 1989), 'president'] = 'Garcia'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1990) & \\\n",
    "           (speech['year'].astype('int32') <= 2000), 'president'] = 'Fujimori'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2001) & \\\n",
    "           (speech['year'].astype('int32') <= 2005), 'president'] = 'Toledo'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2006) & \\\n",
    "           (speech['year'].astype('int32') <= 2010), 'president'] = 'Garcia'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2011) & \\\n",
    "           (speech['year'].astype('int32') <= 2015), 'president'] = 'Humala'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2016) & \\\n",
    "           (speech['year'].astype('int32') <= 2017), 'president'] = 'Kuzcynski'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2018), 'president'] = 'Vizcarra'\n",
    "\n",
    "speech['year-president'] = speech['year'] + '-' + speech['president']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>raw text</th>\n",
       "      <th>year</th>\n",
       "      <th>cleaned text</th>\n",
       "      <th>administration</th>\n",
       "      <th>president</th>\n",
       "      <th>year-president</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mensaje-1963-fbt.txt</td>\n",
       "      <td>[MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PER...</td>\n",
       "      <td>1963</td>\n",
       "      <td>MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PERÚ...</td>\n",
       "      <td>Belaunde(1)</td>\n",
       "      <td>Belaunde</td>\n",
       "      <td>1963-Belaunde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mensaje-1964-fbt.txt</td>\n",
       "      <td>[MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PER...</td>\n",
       "      <td>1964</td>\n",
       "      <td>MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PERÚ...</td>\n",
       "      <td>Belaunde(1)</td>\n",
       "      <td>Belaunde</td>\n",
       "      <td>1964-Belaunde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mensaje-1965-fbt.txt</td>\n",
       "      <td>[MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PER...</td>\n",
       "      <td>1965</td>\n",
       "      <td>MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PERÚ...</td>\n",
       "      <td>Belaunde(1)</td>\n",
       "      <td>Belaunde</td>\n",
       "      <td>1965-Belaunde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mensaje-1966-fbt.txt</td>\n",
       "      <td>[MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PER...</td>\n",
       "      <td>1966</td>\n",
       "      <td>MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PERÚ...</td>\n",
       "      <td>Belaunde(1)</td>\n",
       "      <td>Belaunde</td>\n",
       "      <td>1966-Belaunde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mensaje-1967-fbt.txt</td>\n",
       "      <td>[MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PER...</td>\n",
       "      <td>1967</td>\n",
       "      <td>MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PERÚ...</td>\n",
       "      <td>Belaunde(1)</td>\n",
       "      <td>Belaunde</td>\n",
       "      <td>1967-Belaunde</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename                                           raw text  \\\n",
       "0  mensaje-1963-fbt.txt  [MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PER...   \n",
       "1  mensaje-1964-fbt.txt  [MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PER...   \n",
       "2  mensaje-1965-fbt.txt  [MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PER...   \n",
       "3  mensaje-1966-fbt.txt  [MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PER...   \n",
       "4  mensaje-1967-fbt.txt  [MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PER...   \n",
       "\n",
       "   year                                       cleaned text administration  \\\n",
       "0  1963  MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PERÚ...    Belaunde(1)   \n",
       "1  1964  MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PERÚ...    Belaunde(1)   \n",
       "2  1965  MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PERÚ...    Belaunde(1)   \n",
       "3  1966  MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PERÚ...    Belaunde(1)   \n",
       "4  1967  MENSAJE DEL PRESIDENTE CONSTITUCIONAL DEL PERÚ...    Belaunde(1)   \n",
       "\n",
       "  president year-president  \n",
       "0  Belaunde  1963-Belaunde  \n",
       "1  Belaunde  1964-Belaunde  \n",
       "2  Belaunde  1965-Belaunde  \n",
       "3  Belaunde  1966-Belaunde  \n",
       "4  Belaunde  1967-Belaunde  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech['tokenized_words'] = speech['cleaned text'].apply(lambda x: clean.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we normalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech['normalized_words'] = speech['tokenized_words'].apply(lambda x: clean.normalize_tokens(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gobierno', 1997),\n",
       " ('perú', 1949),\n",
       " ('millón', 1697),\n",
       " ('nacional', 1567),\n",
       " ('peruano', 1290),\n",
       " ('social', 1052),\n",
       " ('pueblo', 1000),\n",
       " ('económico', 964),\n",
       " ('desarrollo', 952),\n",
       " ('año', 864),\n",
       " ('público', 809),\n",
       " ('sector', 809),\n",
       " ('obra', 800),\n",
       " ('inversión', 767),\n",
       " ('ley', 757),\n",
       " ('empresa', 744),\n",
       " ('proyecto', 736),\n",
       " ('servicio', 735),\n",
       " ('programa', 694),\n",
       " ('deber', 693),\n",
       " ('permitir', 683),\n",
       " ('acción', 680),\n",
       " ('política', 667),\n",
       " ('sistema', 632),\n",
       " ('importante', 619),\n",
       " ('proceso', 619),\n",
       " ('sol', 601),\n",
       " ('político', 570),\n",
       " ('fuerza', 557),\n",
       " ('revolución', 548),\n",
       " ('reforma', 543),\n",
       " ('dólares', 538),\n",
       " ('salud', 536),\n",
       " ('recurso', 535),\n",
       " ('esfuerzo', 511),\n",
       " ('economía', 509),\n",
       " ('congreso', 502),\n",
       " ('educación', 498),\n",
       " ('lograr', 495),\n",
       " ('producción', 494),\n",
       " ('internacional', 486),\n",
       " ('mes', 474),\n",
       " ('medida', 471),\n",
       " ('problema', 469),\n",
       " ('construcción', 440),\n",
       " ('nivel', 417),\n",
       " ('plan', 405),\n",
       " ('vida', 405),\n",
       " ('poder', 403),\n",
       " ('región', 399),\n",
       " ('ministerio', 397),\n",
       " ('situación', 395),\n",
       " ('justicia', 390),\n",
       " ('interés', 383),\n",
       " ('cumplir', 378),\n",
       " ('derecho', 373),\n",
       " ('crecimiento', 370),\n",
       " ('nación', 368),\n",
       " ('trabajador', 367),\n",
       " ('banco', 366),\n",
       " ('iniciar', 363),\n",
       " ('privado', 363),\n",
       " ('objetivo', 360),\n",
       " ('alto', 358),\n",
       " ('lima', 357),\n",
       " ('venir', 356),\n",
       " ('población', 356),\n",
       " ('crear', 355),\n",
       " ('revolucionario', 355),\n",
       " ('actividad', 354),\n",
       " ('país', 352),\n",
       " ('presidente', 348),\n",
       " ('república', 346),\n",
       " ('futuro', 346),\n",
       " ('democracia', 344),\n",
       " ('grupo', 344),\n",
       " ('llegar', 340),\n",
       " ('armado', 334),\n",
       " ('central', 331),\n",
       " ('crédito', 331),\n",
       " ('necesario', 330),\n",
       " ('precio', 328),\n",
       " ('regional', 328),\n",
       " ('seguridad', 322),\n",
       " ('zona', 319),\n",
       " ('mantener', 318),\n",
       " ('carretera', 318),\n",
       " ('poner', 317),\n",
       " ('tener', 316),\n",
       " ('construir', 315),\n",
       " ('tarea', 312),\n",
       " ('comunidad', 310),\n",
       " ('constituir', 310),\n",
       " ('centro', 309),\n",
       " ('querer', 309),\n",
       " ('ciudadano', 307),\n",
       " ('producto', 307),\n",
       " ('fundamental', 306),\n",
       " ('realidad', 305),\n",
       " ('libertad', 305)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countsDict = {}\n",
    "for word in speech['normalized_words'].sum():\n",
    "    word = word.lower()\n",
    "    if word in countsDict:\n",
    "        countsDict[word] += 1\n",
    "    else:\n",
    "        countsDict[word] = 1\n",
    "word_counts = sorted(countsDict.items(), key = lambda x : x[1], reverse = True)\n",
    "word_counts[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we tokenize sentences using the function from `nltk` for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech['tokenized_sentences'] = speech['cleaned text'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we tokenize each word in each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "speech['tokenized_words_in_sentences'] = speech['tokenized_sentences'].apply(lambda x: [clean.word_tokenize(s) for s in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we normalized each tokenized word within each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-685b0854c44b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspeech\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'normalized_words_in_sentences'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspeech\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized_words_in_sentences'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4133\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4134\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4135\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4137\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-685b0854c44b>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspeech\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'normalized_words_in_sentences'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspeech\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized_words_in_sentences'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-685b0854c44b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspeech\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'normalized_words_in_sentences'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspeech\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized_words_in_sentences'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "speech['normalized_words_in_sentences'] = speech['tokenized_words_in_sentences'].apply(lambda x: [clean.normalize_tokens(s, stop_words) for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
