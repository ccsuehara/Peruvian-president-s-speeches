{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "This notebook reads the `TXT` files of the speeches and builds a single dataframe with every tokenized and normalized content we'll use.\n",
    "\n",
    "Please note that none of the code chunks of this notebook were actually ran from here. As the processing part took a great amount of time to be completed, we transformed this notebook in a `Python` script and submitted it through the slurm work manager of the Computer Science department. The script is the file `2_data_preparation.py` in this same directory, and the file `run_data_preparation.sbatch` loads it to the slurm environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#from lucem_illud_2020 import word_tokenize\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../../scripts')\n",
    "import data_cleaning as clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the directory of the speeches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_dir = '../../../data/presidentialSpeechPeru/txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the corpus from this path using a helper function we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_raw = clean.loadcorpus(speeches_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we load the result in a data frame and start adding some metadata columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = pd.DataFrame()\n",
    "filenames = []\n",
    "raw = []\n",
    "for filename, raw_speech in speeches_raw.items():\n",
    "    print(filename)\n",
    "    filenames.append(filename)\n",
    "    raw.append(raw_speech)\n",
    "speech['filename'] = filenames\n",
    "speech['raw text'] = raw\n",
    "\n",
    "pattern = re.compile('[0-9]{4}')\n",
    "speech['year'] = speech['filename'].apply(lambda x: pattern.search(x).group(0))\n",
    "speech = speech.sort_values(by='year').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we clean these raw texts using another ad-hoc function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech['cleaned text'] = speech['raw text'].apply(lambda x: clean.clean_raw_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the administration and president of each speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech.loc[(speech['year'].astype('int32') >= 1956) & \\\n",
    "           (speech['year'].astype('int32') <= 1961), 'administration'] = 'Prado'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1962) & \\\n",
    "           (speech['year'].astype('int32') <= 1962), 'administration'] = 'Lindley'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1963) & \\\n",
    "           (speech['year'].astype('int32') <= 1968), 'administration'] = 'Belaunde(1)'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1969) & \\\n",
    "           (speech['year'].astype('int32') <= 1975), 'administration'] = 'Velasco'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1976) & \\\n",
    "           (speech['year'].astype('int32') <= 1979), 'administration'] = 'Morales Bermudez'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1980) & \\\n",
    "           (speech['year'].astype('int32') <= 1984), 'administration'] = 'Belaunde(2)'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1985) & \\\n",
    "           (speech['year'].astype('int32') <= 1989), 'administration'] = 'Garcia(1)'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1990) & \\\n",
    "           (speech['year'].astype('int32') <= 1994), 'administration'] = 'Fujimori(1)'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1995) & \\\n",
    "           (speech['year'].astype('int32') <= 2000), 'administration'] = 'Fujimori(2)'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2001) & \\\n",
    "           (speech['year'].astype('int32') <= 2005), 'administration'] = 'Toledo'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2006) & \\\n",
    "           (speech['year'].astype('int32') <= 2010), 'administration'] = 'Garcia(2)'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2011) & \\\n",
    "           (speech['year'].astype('int32') <= 2015), 'administration'] = 'Humala'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2016), 'administration'] = 'Kuzcynski/Vizcarra'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1956) & \\\n",
    "           (speech['year'].astype('int32') <= 1961), 'president'] = 'Prado'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1962) & \\\n",
    "           (speech['year'].astype('int32') <= 1962), 'president'] = 'Lindley'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1963) & \\\n",
    "           (speech['year'].astype('int32') <= 1968), 'president'] = 'Belaunde'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1969) & \\\n",
    "           (speech['year'].astype('int32') <= 1975), 'president'] = 'Velasco'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1976) & \\\n",
    "           (speech['year'].astype('int32') <= 1979), 'president'] = 'Morales Bermudez'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1980) & \\\n",
    "           (speech['year'].astype('int32') <= 1984), 'president'] = 'Belaunde'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1985) & \\\n",
    "           (speech['year'].astype('int32') <= 1989), 'president'] = 'Garcia'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 1990) & \\\n",
    "           (speech['year'].astype('int32') <= 2000), 'president'] = 'Fujimori'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2001) & \\\n",
    "           (speech['year'].astype('int32') <= 2005), 'president'] = 'Toledo'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2006) & \\\n",
    "           (speech['year'].astype('int32') <= 2010), 'president'] = 'Garcia'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2011) & \\\n",
    "           (speech['year'].astype('int32') <= 2015), 'president'] = 'Humala'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2016) & \\\n",
    "           (speech['year'].astype('int32') <= 2017), 'president'] = 'Kuzcynski'\n",
    "\n",
    "speech.loc[(speech['year'].astype('int32') >= 2018), 'president'] = 'Vizcarra'\n",
    "\n",
    "speech['year-president'] = speech['year'] + '-' + speech['president']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech['tokenized_words'] = speech['cleaned text'].apply(lambda x: clean.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we normalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech['normalized_words'] = speech['tokenized_words'].apply(lambda x: clean.normalize_tokens(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countsDict = {}\n",
    "for word in speech['normalized_words'].sum():\n",
    "    word = word.lower()\n",
    "    if word in countsDict:\n",
    "        countsDict[word] += 1\n",
    "    else:\n",
    "        countsDict[word] = 1\n",
    "word_counts = sorted(countsDict.items(), key = lambda x : x[1], reverse = True)\n",
    "word_counts[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we tokenize sentences using the function from `nltk` for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech['tokenized_sentences'] = speech['cleaned text'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we tokenize each word in each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "speech['tokenized_words_in_sentences'] = speech['tokenized_sentences'].apply(lambda x: [clean.word_tokenize(s) for s in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we normalized each tokenized word within each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech['normalized_words_in_sentences'] = speech['tokenized_words_in_sentences'].apply(lambda x: [clean.normalize_tokens(s, stop_words) for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
